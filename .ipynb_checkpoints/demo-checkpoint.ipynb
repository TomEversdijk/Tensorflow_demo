{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will get familiar tensorflow:keras: https://keras.io/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential, losses, optimizers, utils\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a model.\n",
    "- The input shape will 1x28x28 because the image is 28x28\n",
    "    - Activation:\n",
    "        - relu: Hidden layer ot use\n",
    "        - softmax: Output layer by classification\n",
    "- Conv2D: is a convolutional 2D layer that will be used as an input layer\n",
    "- MaxPooling2D: is a pooling layer\n",
    "- Dropout: will randomly set some weight to 0, this can be used to enhance precision\n",
    "    - Percentage of weights that should be dropped\n",
    "- Flatten: Will flatten our 2D shape into a 1D shape\n",
    "- Dense: A filly connected layer\n",
    "- Compile: The neural network will compile into a tensorflow graph\n",
    "    - loss: the used loss function (most of the time: root mean squared error)\n",
    "    - optimizer: The used optimizer (most of the time: sgd = Stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, 28, 28)  # (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (28, 28, 1)  # (img_rows, img_cols, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss=losses.categorical_crossentropy,\n",
    "                  optimizer=\"sgd\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: We have to prepare the X & Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_data(x_unprepared_data, printing=True):\n",
    "    if printing:\n",
    "        print(\"unprepared X_shape:\" + str(x_unprepared_data.shape))\n",
    "\n",
    "    img_rows = x_unprepared_data.shape[1]\n",
    "    img_cols = x_unprepared_data.shape[2]\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_data = x_unprepared_data.reshape(x_unprepared_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_data = x_unprepared_data.reshape(x_unprepared_data.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "    x_data = x_data.astype('float32')\n",
    "    x_data /= 255\n",
    "    if printing:\n",
    "        print('prepared X_data shape:', x_data.shape)\n",
    "        print(\"\")\n",
    "    return x_data\n",
    "\n",
    "def prepare_Y_data(y_unpreprared_data, printing=True):\n",
    "    if printing:\n",
    "        print(\"unprepared Y_shape:\" + str(y_unpreprared_data.shape))\n",
    "    y_data = utils.to_categorical(y_unpreprared_data, 10)\n",
    "    if printing:\n",
    "        print('prepared Y_data shape:', y_data.shape)\n",
    "        print(\"\")\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a method to randomly select a test item. This test item will be predicted and matched against the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predict(model):\n",
    "    (_, _), (x_test, y_test) = mnist.load_data()\n",
    "    index = random.randint(0, len(x_test) - 1)\n",
    "    X = x_test[index]\n",
    "    image = np.array(X, dtype='float')\n",
    "    pixels = image.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    X = prepare_X_data(x_unprepared_data=np.array([X]),printing=False)\n",
    "    prediction = model.predict(X)\n",
    "    print(\"Prediction array: \" + str(prediction))\n",
    "    print(\"Prediction: \" + str(np.argmax(prediction)))\n",
    "    print(\"Ground truth: \" + str(y_test[index]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the above methods to train the model. \n",
    "Loading de MNIST-dataset and split this dataset into training and testing, this method will only use the training data.\n",
    "After training we will save the model in order to not lose our training progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()\n",
    "    x_prep_train = prepare_X_data(x_unprepared_data=x_train)\n",
    "    y_prep_train = prepare_Y_data(y_unpreprared_data=y_train)\n",
    "    model.fit(x_prep_train, y_prep_train, batch_size=128, epochs=12, verbose=1)\n",
    "    print(\"save model\")\n",
    "    print(\"\")\n",
    "    save_model(model=model, filepath=\"example_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we have to evaluate our training model with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    (_, _), (x_test, y_test) = mnist.load_data()\n",
    "    x_prep_test = prepare_X_data(x_unprepared_data=x_test, printing=False)\n",
    "    y_prep_test = prepare_Y_data(y_unpreprared_data=y_test, printing=False)\n",
    "    score = model.evaluate(x_prep_test, y_prep_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can use all the methods to build, train and test the model. \n",
    "10 randomly selected items will be predicted to see the prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "# train_model(model=model)\n",
    "model = load_model(\"example_model.h5\")\n",
    "eval_model(model)\n",
    "for _ in range(10):\n",
    "    random_predict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correct example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "image = x_test[1]\n",
    "image = np.array(image, dtype='float')\n",
    "pixels = image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "Y = prepare_X_data(x_unprepared_data=np.array([image]), printing=False)\n",
    "prediction = model.predict(Y)\n",
    "print(\"Prediction array: \" + str(prediction))\n",
    "print(\"Prediction: \" + str(np.argmax(prediction)))\n",
    "print(\"Ground truth: \" + str(y_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A incorrect example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "first_image = x_test[591]\n",
    "first_image = np.array(first_image, dtype='float')\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "Y = prepare_X_data(x_unprepared_data=np.array([image]),printing=False)\n",
    "prediction = model.predict(Y)\n",
    "print()\n",
    "print(\"Prediction array: \" + str(prediction))\n",
    "print(\"Prediction: \" + str(np.argmax(prediction)))\n",
    "print(\"Ground truth: \" + str(y_test[591]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases the above functions will be enough. \n",
    "However if we have enormous data we cannot load into memory, we have to make use of a generator as show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator can return results when the result is needed. If the next result is queried, the generator will calculate the next result and return this next result. \n",
    "First we define a helper function to loop over a list indefinite (get_part_of_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_list(lst, start, items):\n",
    "    end = len(lst)\n",
    "    if start + items > end:\n",
    "        missing = start + items - end\n",
    "        result = lst[start:end]\n",
    "        result = np.concatenate((result, lst[:missing]))\n",
    "        return result, missing\n",
    "    else:\n",
    "        return lst[start:start + items], start + items\n",
    "\n",
    "\n",
    "def train_generator(iterations=500):\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()  # Do not load all data in memory at once!\n",
    "    iteration = 0\n",
    "    start = 0\n",
    "    batch_size = 128\n",
    "    while True:\n",
    "        if iterations == iteration:\n",
    "            break\n",
    "        x_unprep_subset, _ = get_part_of_list(lst=x_train, start=start, items=batch_size)\n",
    "        y_unprep_subset, next_start = get_part_of_list(lst=y_train, start=start, items=batch_size)\n",
    "        start = next_start\n",
    "        x_prep_subset = prepare_X_data(x_unprepared_data=x_unprep_subset, printing=False)\n",
    "        y_prep_subset = prepare_Y_data(y_unpreprared_data=y_unprep_subset, printing=False)\n",
    "        yield x_prep_subset, y_prep_subset\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "def test_generator(iterations=100):\n",
    "    (_, _), (x_test, y_test) = mnist.load_data()  # Do not load all data in memory at once!\n",
    "    iteration = 0\n",
    "    start = 0\n",
    "    batch_size = 128\n",
    "    while True:\n",
    "        if iterations == iteration:\n",
    "            break\n",
    "        x_unprep_subset, _ = get_part_of_list(lst=x_test, start=start, items=batch_size)\n",
    "        y_unprep_subset, next_start = get_part_of_list(lst=y_test, start=start, items=batch_size)\n",
    "        start = next_start\n",
    "        x_prep_subset = prepare_X_data(x_unprepared_data=x_unprep_subset,printing=False)\n",
    "        y_prep_subset = prepare_Y_data(y_unpreprared_data=y_unprep_subset, printing=False)\n",
    "        yield x_prep_subset, y_prep_subset\n",
    "        iteration += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will redefine our training & testing functions to use the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_generator(model):\n",
    "    train_gen = train_generator(iterations=500*12)\n",
    "    model.fit(x=train_gen)\n",
    "    print(\"save model\")\n",
    "    print(\"\")\n",
    "    save_model(model=model, filepath=\"example_model_generator.h5\")\n",
    "\n",
    "\n",
    "def eval_model_with_generator(model):\n",
    "    test_gen = test_generator(iterations=100)\n",
    "    score = model.evaluate(x=test_gen)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will combine the generator functions to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "#train_model_with_generator(model=model)\n",
    "model = load_model(\"example_model_generator.h5\")\n",
    "eval_model_with_generator(model)\n",
    "for _ in range(10):\n",
    "    random_predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_demo_env",
   "language": "python",
   "name": "tensorflow_demo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
